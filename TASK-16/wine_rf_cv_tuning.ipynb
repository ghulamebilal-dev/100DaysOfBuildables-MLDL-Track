{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e67ae88",
   "metadata": {},
   "source": [
    "# Random Forest Classification on UCI Wine Dataset\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. Loading the UCI Wine dataset (via `sklearn`).\n",
    "2. Evaluating a Random Forest classifier using K-Fold Cross-Validation.\n",
    "3. Performing hyperparameter tuning with `GridSearchCV`.\n",
    "4. Comparing pre- and post-tuning performance and reflecting on results.\n",
    "\n",
    "Dataset source: UCI ML Repository â€” Wine dataset (loaded through `sklearn.datasets.load_wine`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e93720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "data = load_wine(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "df = data.frame\n",
    "\n",
    "print('Features shape:', X.shape)\n",
    "print('Target distribution:\\n', y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Random Forest with K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "cv_scores = cross_val_score(rf, X, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "print('CV Accuracy scores:', np.round(cv_scores, 4))\n",
    "print('Mean CV Accuracy: {:.4f}'.format(cv_scores.mean()))\n",
    "print('Std CV Accuracy: {:.4f}'.format(cv_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for detailed metrics (baseline)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print('Test Accuracy (baseline): {:.4f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('\\nClassification Report (baseline):\\n', classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix:\\n', cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.title('Baseline Random Forest Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                    param_grid,\n",
    "                    cv=5,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best CV score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae06ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best estimator on the hold-out test set\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# If we want to retrain on train set only:\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "print('Test Accuracy (tuned): {:.4f}'.format(accuracy_score(y_test, y_pred_tuned)))\n",
    "print('\\nClassification Report (tuned):\\n', classification_report(y_test, y_pred_tuned))\n",
    "cm2 = confusion_matrix(y_test, y_pred_tuned)\n",
    "print('\\nConfusion Matrix (tuned):\\n', cm2)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Greens', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.title('Tuned Random Forest Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecff3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pre- and post-tuning\n",
    "baseline_acc = accuracy_score(y_test, y_pred)\n",
    "tuned_acc = accuracy_score(y_test, y_pred_tuned)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Test Accuracy'],\n",
    "    'Baseline': [baseline_acc],\n",
    "    'Tuned': [tuned_acc],\n",
    "    'Absolute Improvement': [tuned_acc - baseline_acc]\n",
    "})\n",
    "comparison.style.format({'Baseline':'{:.4f}','Tuned':'{:.4f}','Absolute Improvement':'{:.4f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ef555",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "\"\n",
    "- **What I did:** Loaded the UCI Wine dataset, trained a Random Forest classifier and evaluated it using 5-fold cross-validation to examine consistency. Then I performed hyperparameter tuning using GridSearchCV to find better hyperparameters.\n",
    "\n",
    "\"\n",
    "- **Pre-tuning performance:** The baseline Random Forest (default hyperparameters) produced the cross-validation mean accuracy shown above and a baseline test accuracy.\n",
    "\n",
    "\"\n",
    "- **Post-tuning performance:** After GridSearchCV, the best parameters are reported and applying the tuned model to the hold-out test set produced a (typically) improved accuracy. The notebook includes the confusion matrices and classification reports for both baseline and tuned models so you can see where performance changed (which classes improved or declined).\n",
    "\n",
    "\"\n",
    "- **How tuning impacted performance:** GridSearchCV searches a grid of hyperparameters and can improve generalization by finding a better bias-variance tradeoff (e.g., adjusting tree depth, number of trees, and leaf sizes). If you see a positive absolute improvement in test accuracy, tuning helped. If improvement is small or zero, the dataset may already be well-suited to the baseline or more extensive search (or RandomizedSearchCV) could be used.\n",
    "\n",
    "\"\n",
    "- **Next steps / suggestions:**\n",
    "\"\n",
    "1. Try `RandomizedSearchCV` for larger parameter spaces to save time.\n",
    "\"\n",
    "2. Evaluate with other metrics (F1 macro/weighted) if class imbalance is a concern.\n",
    "\"\n",
    "3. Consider using `XGBoost` or `LightGBM` (often stronger on tabular data) if you can install them.\n",
    "\"\n",
    "4. Use a nested cross-validation loop if you want an unbiased estimate of generalization performance after tuning.\n",
    "\n",
    "\"\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
